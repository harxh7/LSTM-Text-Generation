{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c2beb7-ac6d-467f-92cf-e269c571ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead7a66f-d352-4284-a968-9555df937d79",
   "metadata": {},
   "source": [
    "Dataset downloaded from kaggle\n",
    "\n",
    "link for dataset : \"https://www.kaggle.com/datasets/fadhilawaliakusuma/alice-in-wonderland\" \n",
    "\n",
    "I chose Alice in Wonderland dataset because it does not have any copyrights. \n",
    "And I will upload the files directly into my GitHub Repo so you can direclty download it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34216dca-329f-4e17-952d-72a74ae3ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Importing Data :-\n",
    "\n",
    "file_paths = [\"../data/books.veryshort.txt\",\n",
    "              \"../data/pride-prejudice.txt\",\n",
    "             \"../data/the-book-thief.txt\"]\n",
    "text = \"\"\n",
    "\n",
    "for path in file_paths:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        text += file.read() + \" \"\n",
    "\n",
    "print(\"Dataset Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc13f44-91f6-4059-83da-6faf3a46765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 7082\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(num_words = 12000)\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(\"Total Vocabulary Size:\", total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5773643d-f4c7-4cdf-bdbb-77bf9ea886f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into token sequence :-\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# Create input-output sequences :-\n",
    "sequence_length = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08cefd91-35d6-4f68-b6f8-dc88b2b64720",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for i in range(sequence_length, len(token_list)):\n",
    "    n_gram_sequence = token_list[i-sequence_length:i+1]\n",
    "    input_sequences.append(n_gram_sequence)\n",
    "\n",
    "input_sequences = np.array(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6f541c-fb06-4608-8af3-805d3e71615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Complete!\n"
     ]
    }
   ],
   "source": [
    "# Split predictors and label :-\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "print(\"Preprocessing Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6683b3fd-8baf-4a2e-be01-fa90bd21e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 59478\n",
      "Validation Samples: 6609\n"
     ]
    }
   ],
   "source": [
    "# Train Validation Split :-\n",
    "split = int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "print(\"Training Samples:\", len(X_train))\n",
    "print(\"Validation Samples:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e27df1-8dcb-49ad-9af0-30364f4c37b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Building Model :-\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(total_words, 100, input_length=sequence_length))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1962ad67-859c-4b9f-be8d-091ea3f9bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 302ms/step - accuracy: 0.0484 - loss: 6.7565 - val_accuracy: 0.0657 - val_loss: 7.2588\n",
      "Epoch 2/10\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 331ms/step - accuracy: 0.0568 - loss: 6.3366 - val_accuracy: 0.0716 - val_loss: 7.3482\n",
      "Epoch 3/10\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 304ms/step - accuracy: 0.0636 - loss: 6.1273 - val_accuracy: 0.0731 - val_loss: 7.2821\n",
      "Epoch 4/10\n",
      "\u001b[1m465/465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 294ms/step - accuracy: 0.0750 - loss: 5.9558 - val_accuracy: 0.0778 - val_loss: 7.3526\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Training Model\"\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3971e20-42c2-4f31-abbe-bf187573d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation func :-\n",
    "\n",
    "\n",
    "def sample_with_temperature(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(seed_text, next_words=50):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=sequence_length, padding='pre')\n",
    "\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted)\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f5941cb-1486-4e2e-8ab0-91a3f360414d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Text Sample 1 ---\n",
      "harry looked at the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "--- Generated Text Sample 2 ---\n",
      "the dark lord the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "--- Generated Text Sample 3 ---\n",
      "hermione said that the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "# Sample text :-\n",
    "seed_1 = \"alice looked at\"\n",
    "seed_2 = \"the white rabbit\"\n",
    "seed_3 = \"she began to\"\n",
    "\n",
    "print(\"\\n--- Generated Text Sample 1 ---\")\n",
    "print(generate_text(seed_1, 40))\n",
    "\n",
    "print(\"\\n--- Generated Text Sample 2 ---\")\n",
    "print(generate_text(seed_2, 40))\n",
    "\n",
    "print(\"\\n--- Generated Text Sample 3 ---\")\n",
    "print(generate_text(seed_3, 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4effd7-cf8a-431b-9e3a-490ecd6c1882",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "In this project, an LSTM-based text generation model was developed and trained on a large textual dataset to generate coherent and meaningful text. Through preprocessing steps such as lowercasing, tokenization, and sequence generation, the dataset was transformed into structured input-output pairs suitable for training a deep learning model. The embedding layer helped represent words in dense vector form, while stacked LSTM layers captured contextual relationships and long-term dependencies within the text.\n",
    "\n",
    "Using sparse categorical crossentropy improved memory efficiency by avoiding one-hot encoding, making it feasible to train the model on a large vocabulary. After training, the model was able to generate new text based on a given seed sequence, demonstrating its ability to learn language structure and word patterns effectively.\n",
    "\n",
    "Experiments with different sequence lengths and model configurations showed that deeper architectures improved contextual understanding but required more computational resources. Overall, this project highlights the effectiveness of LSTM networks in sequence modeling and text generation tasks, while also emphasizing the importance of preprocessing, model optimization, and resource management in building practical NLP systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
